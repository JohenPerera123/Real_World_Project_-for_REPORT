{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvIQ5rM92oClnr9I2B6pae",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JohenPerera123/Real_World_Project_-for_REPORT/blob/main/GPA_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfrraqkH5peN"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score"
      ],
      "metadata": {
        "id": "5m5xeRoM5zW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = \"/content/drive/MyDrive/student_lifestyle_dataset.csv\""
      ],
      "metadata": {
        "id": "r77p7mZo5-kZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(file)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "g8mw_paR6Av7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "XTssxgVN6C93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the descriptive statistics of the dataset\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "P6woW5yG6Mve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check get the the null value count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "_1MqpdQY6RnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check duplicate values and get count\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "50tfVifO6VtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Detect ouliers\n"
      ],
      "metadata": {
        "id": "B5A1pZSA6jZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate the basic measures and detect outliers of each column\n",
        "for col in df.columns:\n",
        "\n",
        "    # Skip the Stress_Level column\n",
        "    if col == \"Stress_Level\":\n",
        "        continue\n",
        "\n",
        "    print(\"Column name:\", col)\n",
        "\n",
        "    # calculate mean, median, Q1, Q3, IQR, limits\n",
        "    mean = df[col].mean()\n",
        "    median = df[col].median()\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_limit = Q1 - 1.5 * IQR\n",
        "    upper_limit = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Detect Outliers\n",
        "    outliers = df[(df[col] < lower_limit) | (df[col] > upper_limit)]\n",
        "\n",
        "    print(f\"Mean: {mean}\")\n",
        "    print(f\"Median: {median}\")\n",
        "    print(f\"Q1 (25%): {Q1}\")\n",
        "    print(f\"Q3 (75%): {Q3}\")\n",
        "    print(f\"IQR (Q3 - Q1): {IQR}\")\n",
        "    print(f\"Lower Limit: {lower_limit}\")\n",
        "    print(f\"Upper Limit: {upper_limit}\")\n",
        "    print(f\"Number of Outliers: {len(outliers)}\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "svhTufAZ6afb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Columns to plot (skip Stress_Level)\n",
        "cols_to_plot = [col for col in df.columns if col != \"Stress_Level\"]\n",
        "\n",
        "n = len(cols_to_plot)  # number of plots\n",
        "rows = (n + 2) // 3    # dynamic row count (3 plots per row)\n",
        "\n",
        "plt.figure(figsize=(15, 5 * rows))\n",
        "\n",
        "for i, col in enumerate(cols_to_plot, 1):\n",
        "    plt.subplot(rows, 3, i)\n",
        "    sns.boxplot(y=df[col])\n",
        "    plt.title(col)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "oW88795l6n5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of columns except Stress_Level\n",
        "cols_to_check = [col for col in df.columns if col != \"Stress_Level\"]\n",
        "\n",
        "# Start with full dataset\n",
        "clean_df = df.copy()\n",
        "\n",
        "for col in cols_to_check:\n",
        "    Q1 = clean_df[col].quantile(0.25)\n",
        "    Q3 = clean_df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    lower = Q1 - 1.5 * IQR\n",
        "    upper = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Keep only rows that are NOT outliers\n",
        "    clean_df = clean_df[(clean_df[col] >= lower) & (clean_df[col] <= upper)]\n",
        "\n",
        "# Final cleaned dataset\n",
        "clean_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "clean_df\n"
      ],
      "metadata": {
        "id": "jBphC13b8kIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = clean_df\n",
        "df.head()"
      ],
      "metadata": {
        "id": "hbdhD2FG-CEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Normalize or Standardize Features\n"
      ],
      "metadata": {
        "id": "RcOVh3uw_Tgn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standardization (Z-score)\n",
        "\n",
        "- z=(xâˆ’Î¼â€‹)/Ïƒ\n",
        "- Âµ = mean\n",
        "- Ïƒ = standard deviation"
      ],
      "metadata": {
        "id": "_Uew7SoV_ZMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Select numeric columns\n",
        "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "# Remove GPA from scaling\n",
        "cols_to_scale = [col for col in numeric_cols if col != \"GPA\"]\n",
        "\n",
        "df_standardized = df.copy()\n",
        "\n",
        "# Apply StandardScaler only on selected columns\n",
        "df_standardized[cols_to_scale] = scaler.fit_transform(df[cols_to_scale])\n",
        "\n",
        "df_standardized.head()\n"
      ],
      "metadata": {
        "id": "lLsAjxRQ-OXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalization (0â€“1)\n",
        "\n",
        "- xâ€²=(maxâˆ’min)/(xâˆ’minâ€‹)"
      ],
      "metadata": {
        "id": "VPyCM4_A_v1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "normalizer = MinMaxScaler()\n",
        "\n",
        "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "# Remove GPA from scaling\n",
        "cols_to_scale = [col for col in numeric_cols if col != \"GPA\"]\n",
        "\n",
        "df_normalized = df.copy()\n",
        "df_standardized[cols_to_scale] = normalizer.fit_transform(df[cols_to_scale])\n",
        "\n",
        "\n",
        "df_normalized.head()"
      ],
      "metadata": {
        "id": "O5nFe_oY_qCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Mapping strees level"
      ],
      "metadata": {
        "id": "aY9XVX55BgRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stress_level_mapping = {'Low': 1, 'Moderate': 2, 'High': 3}\n",
        "\n",
        "df_normalized['Stress_Level'] = df_normalized['Stress_Level'].map(stress_level_mapping)\n",
        "\n",
        "df_normalized.head()"
      ],
      "metadata": {
        "id": "8DEFXtqL_2Ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df_normalized\n",
        "df = df.drop(\"Student_ID\", axis=1)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "9ziFG_IoBsV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Final Cleaned Dataset\n",
        "\n",
        "- No missing values\n",
        "\n",
        "- No duplicates\n",
        "\n",
        "- Outliers handled\n",
        "\n",
        "- All features scaled"
      ],
      "metadata": {
        "id": "eGKlnePKHKuf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Analytics Framework\n"
      ],
      "metadata": {
        "id": "gB0BfKdpC2cF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "GmWCJg8NCxSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##New features\n",
        "- df[\"study_stress_ratio\"] = df[\"Study_Hours_Per_Day\"] / (df[\"Stress_Level\"] + 1)\n",
        "- df['Total_Activity_Hours'] = df['Extracurricular_Hours_Per_Day'] + df['Social_Hours_Per_Day'] + df['Physical_Activity_Hours_Per_Day']\n",
        "- df['Study_to_Sleep_Ratio'] = df['Study_Hours_Per_Day'] / df['Sleep_Hours_Per_Day']"
      ],
      "metadata": {
        "id": "A4oZRP7JDMCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"study_stress_ratio\"] = df[\"Study_Hours_Per_Day\"] / (df[\"Stress_Level\"] + 1)\n",
        "df['Total_Activity_Hours'] = df['Extracurricular_Hours_Per_Day'] + df['Social_Hours_Per_Day'] + df['Physical_Activity_Hours_Per_Day']\n",
        "df['Study_to_Sleep_Ratio'] = df['Study_Hours_Per_Day'] / df['Sleep_Hours_Per_Day']\n",
        "df.head()"
      ],
      "metadata": {
        "id": "McRzcQJGC78M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# correlation heatmap\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.heatmap(df.corr(), annot=True, cmap=\"coolwarm\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VFRmk4HwELos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Define Features (X) and Target (y)\n",
        "\n"
      ],
      "metadata": {
        "id": "kHAL83TDFUoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[['Study_Hours_Per_Day',\t'Extracurricular_Hours_Per_Day',\t'Sleep_Hours_Per_Day',\t'Social_Hours_Per_Day',\t'Physical_Activity_Hours_Per_Day', 'Stress_Level', 'study_stress_ratio',\t'Total_Activity_Hours',\t'Study_to_Sleep_Ratio']]\n",
        "\n",
        "y = df['GPA']"
      ],
      "metadata": {
        "id": "ug6XX2E5FUT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Split Trainâ€“Test Data\n"
      ],
      "metadata": {
        "id": "Bl8lTZhAGAvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "uveNUG0KE7Zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_test)\n"
      ],
      "metadata": {
        "id": "EL62MGwJGF0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Scale the Features\n"
      ],
      "metadata": {
        "id": "5Mp2n0vzHfOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit on training data only\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "# Transform test data using the already learned parameters\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "EPgpw0zgG6fS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train Multiple Models\n"
      ],
      "metadata": {
        "id": "3XDeEKDtHuDg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model 1: Linear Regression\n",
        "\n"
      ],
      "metadata": {
        "id": "v9a8GvOOHwEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "linear = LinearRegression()\n",
        "linear.fit(X_train_scaled, y_train)\n",
        "y_pred_linear=linear.predict(X_test_scaled)"
      ],
      "metadata": {
        "id": "TCg2pEX2HkkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model 2: Ridge Regression\n",
        "\n"
      ],
      "metadata": {
        "id": "1d056dJ7H2nj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "ridge = Ridge(alpha=1.0)\n",
        "ridge.fit(X_train_scaled, y_train)\n",
        "y_pred_ridge=ridge.predict(X_test_scaled)"
      ],
      "metadata": {
        "id": "oL_lp-w4H03S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model 3: Lasso Regression"
      ],
      "metadata": {
        "id": "aR6Dzf3KH-St"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "lasso = Lasso(alpha=0.01)\n",
        "lasso.fit(X_train_scaled, y_train)\n",
        "y_pred_lasso=lasso.predict(X_test_scaled)"
      ],
      "metadata": {
        "id": "6tJyrL4hH9hA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model 4: Decision Tree Regressor"
      ],
      "metadata": {
        "id": "Qrc9r2sIIEd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "dt = DecisionTreeRegressor(max_depth=4)  # tune depth to show overfitting\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt=dt.predict(X_test_scaled)"
      ],
      "metadata": {
        "id": "6EYbWoK2H7LW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model 5: Random Forest Regressor\n",
        "\n"
      ],
      "metadata": {
        "id": "Ym5lPNy5IL57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "rf = RandomForestRegressor(n_estimators=200, random_state=42)\n",
        "rf.fit(X_train_scaled, y_train)\n",
        "y_pred_rf = rf.predict(X_test_scaled)"
      ],
      "metadata": {
        "id": "WG52ZondIKVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model 6: Gradient Boosting Regressor\n",
        "\n"
      ],
      "metadata": {
        "id": "Y7OvF0kbIR7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "gbr = GradientBoostingRegressor(random_state=42)\n",
        "gbr.fit(X_train_scaled, y_train)\n",
        "y_pred_gbr = gbr.predict(X_test_scaled)"
      ],
      "metadata": {
        "id": "xRIr1HbYIOzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluate All Models\n"
      ],
      "metadata": {
        "id": "SZxXTS5PIYo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, name):\n",
        "    y_pred_train = model.predict(X_train_scaled if 'Tree' not in name else X_train)\n",
        "    y_pred_test = model.predict(X_test_scaled if 'Tree' not in name else X_test)\n",
        "\n",
        "    return {\n",
        "        'Model': name,\n",
        "        'Train RÂ²': r2_score(y_train, y_pred_train),\n",
        "        'Test RÂ²': r2_score(y_test, y_pred_test),\n",
        "        'Train MSE': mean_squared_error(y_train, y_pred_train),\n",
        "        'Test MSE': mean_squared_error(y_test, y_pred_test)\n",
        "    }\n",
        "\n",
        "results = []\n",
        "results.append(evaluate(linear, 'Linear Regression'))\n",
        "results.append(evaluate(ridge, 'Ridge Regression'))\n",
        "results.append(evaluate(lasso, 'Lasso Regression'))\n",
        "results.append(evaluate(dt, 'Decision Tree Regressor'))\n",
        "results.append(evaluate(rf, 'Random Forest Regressor'))\n",
        "results.append(evaluate(gbr, 'Gradient Boosting Regressor'))\n",
        "\n",
        "model_results = pd.DataFrame(results)\n",
        "model_results"
      ],
      "metadata": {
        "id": "iuwiEv4MIVrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Interpretation: Checking Overfitting & Underfitting"
      ],
      "metadata": {
        "id": "R0B6IgExIq4G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Overfitting\n",
        "\n",
        "When a model learns the training data too well, including noise and minor fluctuations, so it performs poorly on new/unseen data.\n",
        "\n",
        "Train performance >> Test performance (large gap between Train RÂ² and Test RÂ²)\n",
        "\n",
        "Train error is very low, but test error is high.\n",
        "\n",
        "##Underfitting\n",
        "\n",
        "When a model is too simple to capture the underlying patterns in the data, so it performs poorly on both training and test data.\n",
        "\n",
        "Train performance is low, and Test performance is also low\n",
        "\n",
        "Train error and test error are both high.\n",
        "\n",
        "Sometimes Test RÂ² may be slightly higher than Train RÂ² if the model is unstable."
      ],
      "metadata": {
        "id": "1mUXN3EGIwLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Visualization\n"
      ],
      "metadata": {
        "id": "vZkgViKzI2AI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Actual vs Predicted plots\n",
        "\n"
      ],
      "metadata": {
        "id": "vGI1NQ4_I4tq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary of model predictions\n",
        "models = {\n",
        "    \"Linear Regression\": y_pred_linear,\n",
        "    \"Ridge Regression\": y_pred_ridge,\n",
        "    \"Lasso Regression\": y_pred_lasso,\n",
        "    \"Decision Tree\": y_pred_dt,\n",
        "    \"Random Forest\": y_pred_rf,\n",
        "    \"Gradient Boosting\": y_pred_gbr\n",
        "}\n",
        "\n",
        "x = range(len(y_test))\n",
        "\n",
        "# Number of models\n",
        "num_models = len(models)\n",
        "\n",
        "# Create subplots\n",
        "plt.figure(figsize=(15, 20))\n",
        "\n",
        "for i, (model_name, preds) in enumerate(models.items(), 1):\n",
        "    plt.subplot(3, 2, i)  # 3 rows, 2 columns\n",
        "    plt.plot(x, y_test, label='Actual', linewidth=2)\n",
        "    plt.plot(x, preds, label='Predicted', linestyle='--')\n",
        "    plt.title(f\"Actual vs Predicted - {model_name}\")\n",
        "    plt.xlabel(\"Sample Index\")\n",
        "    plt.ylabel(\"GPA\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UjywTIYvIeCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Observations\n",
        "\n",
        "1. Linear, Ridge, Lasso Regression\n",
        "\n",
        "- Train and test RÂ² are similar (~0.53â€“0.54), indicating no overfitting.\n",
        "\n",
        "- Test MSE is slightly higher than train MSE, but overall performance is moderate.\n",
        "\n",
        "2. Decision Tree Regressor\n",
        "\n",
        "- Train RÂ² = 0.558, Test RÂ² = 0.497 â†’ slight overfitting.\n",
        "\n",
        "- Test MSE is higher than linear models, suggesting less generalization.\n",
        "\n",
        "3. Random Forest Regressor\n",
        "\n",
        "- Train RÂ² = 0.930, Test RÂ² = 0.445 â†’ strong overfitting.\n",
        "\n",
        "- Very low train MSE (0.006) but high test MSE (0.051), indicating poor generalization.\n",
        "\n",
        "4. Gradient Boosting Regressor\n",
        "\n",
        "- Train RÂ² = 0.646, Test RÂ² = 0.498 â†’ mild overfitting.\n",
        "\n",
        "- Performs slightly better than Decision Tree on test set but worse than linear models in RÂ² stability.\n",
        "\n",
        "##Key Takeaways\n",
        "\n",
        "- Linear models (Linear, Ridge, Lasso) give stable and consistent results, even if moderate.\n",
        "\n",
        "- Ensemble models like Random Forest overfit the data severely due to small sample size or noisy features.\n",
        "\n",
        "- Gradient Boosting improves slightly over single Decision Tree but still overfits.\n",
        "\n",
        "- For this dataset, simple linear models may generalize better, while more complex models require feature engineering or regularization."
      ],
      "metadata": {
        "id": "Wbqy8M1HJ89I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prediction Error Plots\n",
        "\n"
      ],
      "metadata": {
        "id": "WwistkJlKd3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    \"Linear Regression\": y_pred_linear,\n",
        "    \"Ridge Regression\": y_pred_ridge,\n",
        "    \"Lasso Regression\": y_pred_lasso,\n",
        "    \"Decision Tree\": y_pred_dt,\n",
        "    \"Random Forest\": y_pred_rf,\n",
        "    \"Gradient Boosting\": y_pred_gbr\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15, 20))\n",
        "\n",
        "for i, (name, preds) in enumerate(models.items(), 1):\n",
        "    plt.subplot(3, 2, i)\n",
        "    plt.scatter(y_test, preds, alpha=0.6)\n",
        "    plt.plot([y_test.min(), y_test.max()],\n",
        "             [y_test.min(), y_test.max()],\n",
        "             color='red', linewidth=2)\n",
        "    plt.title(f\"Prediction Error Plot - {name}\")\n",
        "    plt.xlabel(\"Actual GPA\")\n",
        "    plt.ylabel(\"Predicted GPA\")\n",
        "    plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dpQexNoYI9MN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Model             | Expectation on Scatter Plot                                                                                        |\n",
        "| ----------------- | ------------------------------------------------------------------------------------------------------------------ |\n",
        "| Linear Regression | Moderate scatter, points relatively close to line (consistent with Test RÂ² ~0.53)                                  |\n",
        "| Ridge / Lasso     | Very similar to Linear Regression (regularized versions)                                                           |\n",
        "| Decision Tree     | Slightly more spread than linear models, overfitting visible as some points perfectly predicted and others far off |\n",
        "| Random Forest     | Very tight on some train points, but large scatter on test points (overfitting)                                    |\n",
        "| Gradient Boosting | Moderate scatter, slightly better than Decision Tree, but still spread wider than linear models                    |\n",
        "\n",
        "##Summary\n",
        "\n",
        "- Best alignment with red line: Linear, Ridge, Lasso â†’ best generalization.\n",
        "\n",
        "- High variance: Random Forest â†’ overfitting, predictions deviate more from actual.\n",
        "\n",
        "- Intermediate: Decision Tree and Gradient Boosting â†’ slightly overfit but better than Random Forest on test set."
      ],
      "metadata": {
        "id": "zKdK_fJDMLVI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Error Distribution\n",
        "\n"
      ],
      "metadata": {
        "id": "AlKJB--IMlTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 20))\n",
        "\n",
        "for i, (name, preds) in enumerate(models.items(), 1):\n",
        "    plt.subplot(3, 2, i)\n",
        "    residuals = y_test - preds\n",
        "    sns.histplot(residuals, kde=True, bins=20)\n",
        "    plt.title(f\"Error Distribution - {name}\")\n",
        "    plt.xlabel(\"Error (Residual)\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iNMAhtt5KiZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observations from the Error Distribution Plots\n",
        "1. Linear, Ridge, Lasso Regression\n",
        "\n",
        "- Residuals are roughly centered around 0, forming a bell-shaped distribution.\n",
        "\n",
        "- Spread is moderate, indicating consistent prediction errors.\n",
        "\n",
        "- No extreme outliers in residuals â†’ models generalize well.\n",
        "\n",
        "- This aligns with your Test RÂ² (~0.53) and low Test MSE (~0.043).\n",
        "\n",
        "2. Decision Tree\n",
        "\n",
        "- Residuals are skewed to the right with a long tail, meaning some predictions are significantly overestimated.\n",
        "\n",
        "- Some residuals are near 0 (perfect predictions for some points), showing overfitting to training data.\n",
        "\n",
        "- Test performance is worse than linear models (Test RÂ² ~0.497).\n",
        "\n",
        "3. Random Forest\n",
        "\n",
        "- Residuals are mostly centered around 0, but there are some long tails â†’ indicates overfitting on training patterns but struggles to generalize.\n",
        "\n",
        "- Test RÂ² is low (~0.445), showing poor generalization.\n",
        "\n",
        "4. Gradient Boosting\n",
        "\n",
        "- Residuals are centered near 0 with moderate spread.\n",
        "\n",
        "- Slightly better distribution than Decision Tree, but still more variance than linear models.\n",
        "\n",
        "- Test RÂ² ~0.498 â†’ mild overfitting, better than Decision Tree but worse than Linear/Ridge/Lasso.\n",
        "\n",
        "âœ… Key Takeaways\n",
        "\n",
        "1. Linear, Ridge, and Lasso Regression\n",
        "\n",
        "- Most stable, unbiased, and generalizes best.\n",
        "\n",
        "- Residuals are symmetric, centered at 0, and moderate in spread.\n",
        "\n",
        "2. Decision Tree and Gradient Boosting\n",
        "\n",
        "- Show overfitting patterns: spikes at 0 and long tails.\n",
        "\n",
        "3. Random Forest\n",
        "\n",
        "- Severe overfitting with long tails in residuals â†’ poor test performance."
      ],
      "metadata": {
        "id": "N_0nfDggQ1-9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Comparison Table\n",
        "\n"
      ],
      "metadata": {
        "id": "JMSCPrm8RiaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "metrics = {\"Model\": [], \"R2\": [], \"MAE\": [], \"RMSE\": []}\n",
        "\n",
        "for name, preds in models.items():\n",
        "    metrics[\"Model\"].append(name)\n",
        "    metrics[\"R2\"].append(r2_score(y_test, preds))\n",
        "    metrics[\"MAE\"].append(mean_absolute_error(y_test, preds))\n",
        "    metrics[\"RMSE\"].append(np.sqrt(mean_squared_error(y_test, preds)))\n",
        "\n",
        "df_metrics = pd.DataFrame(metrics)\n",
        "df_metrics"
      ],
      "metadata": {
        "id": "IkIRyp-mMoH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Comparison Bar Charts\n",
        "\n"
      ],
      "metadata": {
        "id": "pRWPN1iVRv1y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RÂ², MAE,RMSE Comparison"
      ],
      "metadata": {
        "id": "XupTBj9YRyP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Model names\n",
        "models = [\"Linear Regression\", \"Ridge Regression\", \"Lasso Regression\",\n",
        "          \"Decision Tree\", \"Random Forest\", \"Gradient Boosting\"]\n",
        "\n",
        "# Metrics\n",
        "r2 = [0.532801, 0.533004, 0.533649, -1.568823, 0.444994, 0.498066]\n",
        "mae = [0.167725, 0.167702, 0.167046, 0.410984, 0.182283, 0.174521]\n",
        "rmse = [0.207412, 0.207367, 0.207223, 0.486351, 0.226064, 0.214984]\n",
        "\n",
        "# ----------------- RÂ² Bar Chart -----------------\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.bar(models, r2, color='skyblue')\n",
        "plt.title(\"RÂ² of Models\")\n",
        "plt.ylabel(\"RÂ²\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(axis='y')\n",
        "plt.show()\n",
        "\n",
        "# ----------------- MAE Bar Chart -----------------\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.bar(models, mae, color='salmon')\n",
        "plt.title(\"MAE of Models\")\n",
        "plt.ylabel(\"Mean Absolute Error (MAE)\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(axis='y')\n",
        "plt.show()\n",
        "\n",
        "# ----------------- RMSE Bar Chart -----------------\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.bar(models, rmse, color='lightgreen')\n",
        "plt.title(\"RMSE of Models\")\n",
        "plt.ylabel(\"Root Mean Squared Error (RMSE)\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(axis='y')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dyxkkoGORlru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1ï¸âƒ£ RÂ² (Coefficient of Determination)\n",
        "\n",
        "- Measures how much of the variation in GPA is explained by the model.\n",
        "\n",
        "- Closer to 1 â†’ better model, negative values â†’ worse than predicting the mean.\n",
        "| Model             | RÂ²     | Comment                                                                   |\n",
        "| ----------------- | ------ | ------------------------------------------------------------------------- |\n",
        "| Linear Regression | 0.533  | Explains ~53% of GPA variation. Good and stable.                          |\n",
        "| Ridge Regression  | 0.533  | Almost same as Linear. Regularization didnâ€™t change much.                 |\n",
        "| Lasso Regression  | 0.534  | Slightly better than Linear/Ridge. Best generalization.                   |\n",
        "| Decision Tree     | -1.569 | Very poor. Model is overfitting badly.                                    |\n",
        "| Random Forest     | 0.445  | Moderate. Some overfitting; doesnâ€™t generalize well.                      |\n",
        "| Gradient Boosting | 0.498  | Better than Random Forest and Decision Tree but worse than linear models. |\n",
        "\n",
        "2ï¸âƒ£ MAE (Mean Absolute Error)\n",
        "\n",
        "- Measures average absolute difference between predicted and actual GPA.\n",
        "\n",
        "- Lower MAE â†’ better predictions.\n",
        "| Model             | MAE   | Comment                                                     |\n",
        "| ----------------- | ----- | ----------------------------------------------------------- |\n",
        "| Linear Regression | 0.168 | Small error; good predictions.                              |\n",
        "| Ridge Regression  | 0.168 | Almost identical to Linear.                                 |\n",
        "| Lasso Regression  | 0.167 | Slightly better; lowest MAE.                                |\n",
        "| Decision Tree     | 0.411 | Very high error; predictions are unreliable.                |\n",
        "| Random Forest     | 0.182 | Moderate error; overfitting may cause large deviations.     |\n",
        "| Gradient Boosting | 0.175 | Better than Random Forest; still higher than linear models. |\n",
        "\n",
        "3ï¸âƒ£ RMSE (Root Mean Squared Error)\n",
        "\n",
        "- Measures average magnitude of errors, giving more weight to large errors.\n",
        "\n",
        "- Lower RMSE â†’ better model, sensitive to extreme predictions.\n",
        "| Model             | RMSE  | Comment                                                        |\n",
        "| ----------------- | ----- | -------------------------------------------------------------- |\n",
        "| Linear Regression | 0.207 | Good overall; consistent.                                      |\n",
        "| Ridge Regression  | 0.207 | Very similar to Linear.                                        |\n",
        "| Lasso Regression  | 0.207 | Slightly better than Linear; fewest large errors.              |\n",
        "| Decision Tree     | 0.486 | Very high; some predictions are far from actual.               |\n",
        "| Random Forest     | 0.226 | Moderate; better than Decision Tree, worse than linear models. |\n",
        "| Gradient Boosting | 0.215 | Slightly better than Random Forest; still higher than Lasso.   |\n",
        "\n",
        "\n",
        "1. Best Model: Lasso Regression\n",
        "\n",
        "Best RÂ², lowest MAE, and RMSE.\n",
        "\n",
        "Stable predictions, generalizes well.\n",
        "\n",
        "2. Linear & Ridge Regression\n",
        "\n",
        "Close to Lasso. Slightly worse in MAE/RMSE.\n",
        "\n",
        "3. Ensemble Models (Random Forest, Gradient Boosting)\n",
        "\n",
        "Overfit on training data â†’ worse performance on test data.\n",
        "\n",
        "Could improve with hyperparameter tuning.\n",
        "\n",
        "4. Decision Tree\n",
        "\n",
        "Worst overall; very unstable and overfits easily."
      ],
      "metadata": {
        "id": "xlCq3kaFTyCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "def evaluate_model(model, name, X_train, X_test, y_train, y_test, X_train_scaled=None, X_test_scaled=None):\n",
        "    \"\"\"\n",
        "    Evaluate regression model performance and detect overfitting/underfitting.\n",
        "\n",
        "    Parameters:\n",
        "    - model: trained model\n",
        "    - name: model name (string)\n",
        "    - X_train, X_test, y_train, y_test: training and test data\n",
        "    - X_train_scaled, X_test_scaled: optional scaled data for linear models\n",
        "\n",
        "    Returns:\n",
        "    - dict with RÂ², MSE, and generalization note\n",
        "    \"\"\"\n",
        "\n",
        "    # Use scaled data for non-tree models, raw data for tree-based models\n",
        "    if 'Tree' in name or 'Forest' in name or 'Boosting' in name:\n",
        "        X_tr, X_te = X_train, X_test\n",
        "    else:\n",
        "        X_tr, X_te = X_train_scaled, X_test_scaled\n",
        "\n",
        "    # Predictions\n",
        "    y_pred_train = model.predict(X_tr)\n",
        "    y_pred_test = model.predict(X_te)\n",
        "\n",
        "    # Metrics\n",
        "    train_r2 = r2_score(y_train, y_pred_train)\n",
        "    test_r2 = r2_score(y_test, y_pred_test)\n",
        "    train_mse = mean_squared_error(y_train, y_pred_train)\n",
        "    test_mse = mean_squared_error(y_test, y_pred_test)\n",
        "\n",
        "    # Check generalization\n",
        "    diff = train_r2 - test_r2\n",
        "    if diff > 0.05:\n",
        "        note = \"Overfitting (Train >> Test)\"\n",
        "    elif diff < -0.05:\n",
        "        note = \"Underfitting (Test > Train)\"\n",
        "    else:\n",
        "        note = \"Good generalization\"\n",
        "\n",
        "    return {\n",
        "        'Model': name,\n",
        "        'Train RÂ²': round(train_r2, 3),\n",
        "        'Test RÂ²': round(test_r2, 3),\n",
        "        'Train MSE': round(train_mse, 4),\n",
        "        'Test MSE': round(test_mse, 4),\n",
        "        'Notes': note\n",
        "    }\n",
        "\n",
        "# Evaluate all models\n",
        "results = [\n",
        "    evaluate_model(linear, 'Linear Regression', X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled),\n",
        "    evaluate_model(ridge, 'Ridge Regression', X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled),\n",
        "    evaluate_model(lasso, 'Lasso Regression', X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled),\n",
        "    evaluate_model(dt, 'Decision Tree Regressor', X_train, X_test, y_train, y_test),\n",
        "    evaluate_model(rf, 'Random Forest Regressor', X_train, X_test, y_train, y_test),\n",
        "    evaluate_model(gbr, 'Gradient Boosting Regressor', X_train, X_test, y_train, y_test)\n",
        "]\n",
        "\n",
        "# Convert to DataFrame\n",
        "model_results = pd.DataFrame(results)\n",
        "model_results\n"
      ],
      "metadata": {
        "id": "5jJWEH8rR1Qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Key Metrics Explained\n",
        "\n",
        "###RÂ² (Coefficient of Determination)\n",
        "\n",
        "- Measures how well the model explains the variance in the data.\n",
        "\n",
        "- Closer to 1 â†’ better. Closer to 0 â†’ poor fit. Negative â†’ model is worse than predicting the mean.\n",
        "\n",
        "###MSE (Mean Squared Error)\n",
        "\n",
        "- Measures the average squared difference between predicted and actual values.\n",
        "\n",
        "- Smaller â†’ better.\n",
        "\n",
        "Notes\n",
        "\n",
        "- Indicates whether the model is overfitting, underfitting, or generalizing well.\n",
        "\n",
        "##Model-by-Model Summary\n",
        "\n",
        "###Linear, Ridge, Lasso Regression\n",
        "\n",
        "- RÂ²: ~0.53 â†’ explains about 53% of the variation in GPA.\n",
        "\n",
        "- MSE: ~0.04 â†’ predictions are reasonably close to actual values.\n",
        "\n",
        "Notes: \"Good generalization\" â†’ Train & Test RÂ² are close, so no overfitting or underfitting.\n",
        "\n",
        "Interpretation: These linear models are stable, reliable, and simple, giving consistent results.\n",
        "\n",
        "### Decision Tree Regressor\n",
        "\n",
        "- Train RÂ² = 0.558 (slightly higher than linear models)\n",
        "\n",
        "- Test RÂ² = 0.497 (drop from train â†’ small overfitting)\n",
        "\n",
        "- Train MSE < Test MSE â†’ small overfitting.\n",
        "\n",
        "Notes: \"Overfitting (Train >> Test)\" â†’ tree memorized some patterns in training but doesnâ€™t generalize perfectly.\n",
        "\n",
        "Interpretation: Single decision trees are unstable and can overfit small datasets.\n",
        "\n",
        "### Random Forest Regressor\n",
        "\n",
        "- Train & Test RÂ² are negative â†’ predicts worse than simply taking the mean GPA.\n",
        "\n",
        "- MSE is high â†’ very poor performance.\n",
        "\n",
        "Notes: \"Underfitting\" â†’ model is too conservative; maybe hyperparameters need tuning or dataset is small.\n",
        "\n",
        "### Gradient Boosting Regressor\n",
        "\n",
        "- Train & Test RÂ² are negative but slightly better than Random Forest.\n",
        "\n",
        "- Slight underfitting â†’ model not capturing the patterns well.\n",
        "\n",
        "Could improve with learning rate adjustment, more estimators, or max_depth tuning.\n",
        "\n",
        "##Overall Takeaways\n",
        "\n",
        "### - Best models: Lasso Regression, Linear Regression, Ridge Regression â†’ good balance between Train/Test performance.\n",
        "\n",
        "###Tree-based models (Decision Tree, Random Forest, Gradient Boosting)\n",
        "\n",
        "- Decision Tree: slight overfitting.\n",
        "\n",
        "- Random Forest & Gradient Boosting: underfitting (poor RÂ²).\n",
        "\n",
        "Reason: Dataset may be small or features are mostly linear â†’ linear models perform better."
      ],
      "metadata": {
        "id": "Tru05jzcXOQX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Interpretability & Evaluation\n"
      ],
      "metadata": {
        "id": "a3PHGOTAY3dV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the best model as Ridge Regression\n",
        "best_model = Ridge(alpha=0.2, random_state=42)  # you can adjust alpha\n",
        "\n",
        "# Fit the model\n",
        "best_model.fit(X_train_scaled, y_train)"
      ],
      "metadata": {
        "id": "IAndf4_vVIf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FEATURE IMPORTANCE (FOR BEST MODEL)\n"
      ],
      "metadata": {
        "id": "VhJdgQppZTVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "coefficients = best_model.coef_\n",
        "features = X_train.columns\n",
        "\n",
        "# Create a DataFrame\n",
        "feat_df = pd.DataFrame({'Feature': features, 'Importance': np.abs(coefficients)})\n",
        "feat_df = feat_df.sort_values(by='Importance', ascending=False)\n",
        "print(feat_df)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.barplot(x='Importance', y='Feature', data=feat_df, palette='viridis')\n",
        "plt.title(\"Feature Importance in Lasso Regression\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0IFUyq-PZOKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ridge Regression Feature Importance Analysis\n",
        "\n",
        "## Model Overview\n",
        "\n",
        "You trained a **Ridge Regression** model with:\n",
        "\n",
        "- `alpha = 0.2` (regularization strength)\n",
        "- Features scaled before fitting\n",
        "- Target variable: [your target variable, e.g., GPA]\n",
        "\n",
        "Ridge Regression is a **linear model** that penalizes large coefficients using **L2 regularization**. The `alpha` value controls how strong this penalty is:\n",
        "\n",
        "- **Higher alpha** â†’ stronger penalty â†’ coefficients shrink toward zero\n",
        "- **Lower alpha** â†’ weaker penalty â†’ coefficients closer to ordinary linear regression\n",
        "\n",
        "With `alpha = 0.2`, the penalty is moderate, allowing coefficients to reflect feature importance without being overly shrunk.\n",
        "\n",
        "---\n",
        "\n",
        "## Feature Importance Table\n",
        "\n",
        "| Feature                        | Importance |\n",
        "|--------------------------------|-----------|\n",
        "| Study_Hours_Per_Day             | 0.135783  |\n",
        "| Stress_Level                    | 0.094967  |\n",
        "| Sleep_Hours_Per_Day             | 0.075136  |\n",
        "| study_stress_ratio              | 0.063824  |\n",
        "| Study_to_Sleep_Ratio            | 0.043547  |\n",
        "| Total_Activity_Hours            | 0.040677  |\n",
        "| Extracurricular_Hours_Per_Day   | 0.018979  |\n",
        "| Physical_Activity_Hours_Per_Day | 0.017418  |\n",
        "| Social_Hours_Per_Day            | 0.011150  |\n",
        "\n",
        "**Explanation of Columns:**\n",
        "\n",
        "- **Feature**: Name of the input variable.\n",
        "- **Importance**: Magnitude of the Ridge Regression coefficient (absolute value).  \n",
        "  - Ridge coefficients can be positive or negative.  \n",
        "  - Using `np.abs()` allows us to compare their **relative influence** on the target.  \n",
        "  - Larger values indicate the feature has a stronger effect on the target variable.\n",
        "\n",
        "---\n",
        "\n",
        "## Insights from the Data\n",
        "\n",
        "1. **Most important features:**\n",
        "   - `Study_Hours_Per_Day` (0.1358)\n",
        "   - `Stress_Level` (0.0950)\n",
        "   - `Sleep_Hours_Per_Day` (0.0751)\n",
        "\n",
        "   These are the top predictors affecting the target variable.\n",
        "\n",
        "2. **Moderate importance:**\n",
        "   - `study_stress_ratio` and `Study_to_Sleep_Ratio`  \n",
        "     These derived features still contribute meaningfully.\n",
        "\n",
        "3. **Least important features:**\n",
        "   - `Extracurricular_Hours_Per_Day`, `Physical_Activity_Hours_Per_Day`, `Social_Hours_Per_Day`  \n",
        "     These have minimal influence compared to core academic or sleep/stress-related features.\n",
        "\n",
        "---\n",
        "\n",
        "## Notes on Alpha (0.2)\n",
        "\n",
        "- With `alpha = 0.2`, coefficients are slightly shrunk compared to normal linear regression.\n",
        "- If you increase alpha further, some coefficients could shrink more, reducing their importance.\n",
        "- If you decrease alpha toward 0, the model behaves like ordinary linear regression (no shrinkage).\n",
        "\n",
        "---\n",
        "\n",
        "## Visualization\n",
        "\n",
        "The bar plot shows **absolute coefficient values** to compare relative influence:\n"
      ],
      "metadata": {
        "id": "VJGlN_Zla9RQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SHAP EXPLANATIONS (GLOBAL + INDIVIDUAL)\n",
        "\n"
      ],
      "metadata": {
        "id": "piOPPmQlbOpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "\n",
        "# Create explainer for Ridge Regression\n",
        "explainer = shap.LinearExplainer(best_model, X_train_scaled, feature_perturbation=\"correlation_dependent\")\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "# Summary plot (global importance)\n",
        "shap.summary_plot(shap_values, X_test, feature_names=X_test.columns)\n"
      ],
      "metadata": {
        "id": "qwHu35QZZiEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“ˆ SHAP Feature Importance Summary for Regression Model\n",
        "\n",
        "This summary is based on a SHAP (SHapley Additive exPlanations) summary plot, which illustrates the **global impact** and direction of each feature on the model's output (prediction). The model is a **Ridge Regression** as indicated by the use of `shap.LinearExplainer`.\n",
        "\n",
        "## Key Findings\n",
        "\n",
        "| Rank | Feature Name | Overall Impact (Magnitude) | Direction of Impact |\n",
        "| :---: | :--- | :--- | :--- |\n",
        "| **1** | **Study_Hours_Per_Day** | **Highest** | Primarily drives the model output **higher** (positive SHAP values). |\n",
        "| 2 | **Sleep_Hours_Per_Day** | High | Primarily drives the model output **lower** (negative SHAP values). |\n",
        "| 3 | **Total_Activity_Hours** | High | Primarily drives the model output **lower** (negative SHAP values). |\n",
        "| 4 | **Stress_Level** | Moderate | Primarily drives the model output **higher** (positive SHAP values). |\n",
        "| 5 | **Physical_Activity_Hours_Per_Day** | Moderate | Primarily drives the model output **lower** (negative SHAP values). |\n",
        "\n",
        "---\n",
        "\n",
        "## Detailed Feature Analysis\n",
        "\n",
        "The plot shows the distribution of SHAP values for each feature.\n",
        "\n",
        "* **X-axis (SHAP value):** Represents the feature's contribution to the model's prediction. A **positive SHAP value** means the feature pushes the prediction *higher*, and a **negative SHAP value** means it pushes the prediction *lower*.\n",
        "* **Color (Feature value):** Indicates the actual value of the feature, with **Red** representing a *high* feature value and **Blue** representing a *low* feature value.\n",
        "\n",
        "### ðŸ¥‡ Features with the Strongest Impact\n",
        "\n",
        "The top three features exhibit the largest magnitude of SHAP values, indicating they have the **greatest influence** on the model's prediction.\n",
        "\n",
        "* **Study_Hours_Per_Day (Dominant Driver):**\n",
        "    * This is the **most influential feature**, with SHAP values extending significantly into the positive range (up to $\\approx +3$).\n",
        "    * **High** values (red dots) of `Study_Hours_Per_Day` consistently result in **large positive SHAP values**, meaning *more study hours lead to a higher predicted outcome*.\n",
        "* **Sleep_Hours_Per_Day:**\n",
        "    * The second most influential feature, with SHAP values centered around $\\approx -1$.\n",
        "    * **High** values (red dots) are associated with **negative SHAP values**, suggesting that *more sleep hours lead to a lower predicted outcome*.\n",
        "* **Total_Activity_Hours:**\n",
        "    * Similar in impact to `Sleep_Hours_Per_Day`, with most SHAP values in the negative range.\n",
        "    * **High** values (red dots) of activity hours correspond to **negative SHAP values**, implying *a higher total activity level is associated with a lower predicted outcome*.\n",
        "\n",
        "### ðŸ¥ˆ Features with Moderate Impact\n",
        "\n",
        "These features have SHAP values closer to zero, indicating a smaller, but still noticeable, contribution to the prediction.\n",
        "\n",
        "* **Stress_Level:**\n",
        "    * Pushes the prediction primarily **higher** (positive SHAP values).\n",
        "    * **High** stress levels (red dots) appear to contribute to the **higher** positive SHAP values.\n",
        "* **Physical_Activity_Hours_Per_Day:**\n",
        "    * Pushes the prediction primarily **lower** (negative SHAP values), centered around $\\approx -0.5$.\n",
        "    * **High** hours of physical activity (red dots) generally correlate with **lower** predicted outcomes (negative SHAP values).\n",
        "* **study\\_stress\\_ratio:**\n",
        "    * Has a small positive impact on the model output.\n",
        "\n",
        "### ðŸ¥‰ Features with Minimal Impact\n",
        "\n",
        "The remaining features have SHAP values very close to zero, suggesting they are the **least important** in driving the model's prediction.\n",
        "\n",
        "* `Extracurricular_Hours_Per_Day`\n",
        "* `Social_Hours_Per_Day`\n",
        "* `Study_to_Sleep_Ratio`\n",
        "\n",
        "***\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "The model is overwhelmingly dominated by **Study_Hours_Per_Day** as the main factor *increasing* the prediction, and **Sleep_Hours_Per_Day** and **Total_Activity_Hours** as the main factors *decreasing* the prediction. The effects of the other features are significantly minor by comparison."
      ],
      "metadata": {
        "id": "syquTsVTc-_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SHAP Force Plot (INDIVIDUAL EXPLANATION)\n",
        "\n"
      ],
      "metadata": {
        "id": "lWg5YTBOdSTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Force plot for a single prediction (local explanation)\n",
        "shap.initjs()\n",
        "shap.force_plot(explainer.expected_value, shap_values[0,:], X_test.iloc[0,:])"
      ],
      "metadata": {
        "id": "_bpS09T5bSI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instance-Specific Contribution Analysis\n",
        "\n",
        "This section breaks down the specific feature values that influenced the prediction for this single data point:\n",
        "\n",
        "## ðŸ“ˆ Pushing the Prediction Higher (Red)\n",
        "\n",
        "These feature values were responsible for increasing the predicted outcome above the Base Value:\n",
        "\n",
        "* **Study_Hours_Per_Day = 6.8**: This is the strongest positive factor, contributing the most to increasing the prediction.\n",
        "* **Stress_Level = 2**: This moderate stress level is the second strongest positive contributor.\n",
        "* **study_stress_ratio = 2.267**: Contributed a small push to the positive side.\n",
        "\n",
        "## ðŸ“‰ Pushing the Prediction Lower (Blue)\n",
        "\n",
        "These feature values were responsible for decreasing the predicted outcome below the Base Value:\n",
        "\n",
        "* **Sleep_Hours_Per_Day = 8.6**: This is the strongest negative factor, and its pull is significant enough to nearly negate the positive effect of the study hours.\n",
        "* **Total_Activity_Hours = 8.6**: This is the second strongest negative factor, also strongly driving the prediction down.\n",
        "\n",
        "## âœ… Summary of the Single Prediction\n",
        "\n",
        "For this particular instance, the large positive impact from 6.8 **Study_Hours_Per_Day** was almost perfectly balanced by the large negative impact from 8.6 **Sleep_Hours_Per_Day** and 8.6 **Total_Activity_Hours**. This balance results in a final predicted outcome ($\\text{f(x)}$) that is very close to the overall average Base Value of 3.19.\n"
      ],
      "metadata": {
        "id": "yomL9xT5eo8C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SHAP Summary Plot (GLOBAL EXPLANATION)"
      ],
      "metadata": {
        "id": "iHgn7f0ofDCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "shap.summary_plot(shap_values, X_test, plot_type='bar')"
      ],
      "metadata": {
        "id": "ehBIMYHDdfGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SHAP Model Explanation Summary\n",
        "\n",
        "This document summarizes the insights from the **SHAP Bar Summary Plot (global feature importance)** and the **SHAP Force Plot (local prediction explanation)** for regression model.\n",
        "\n",
        "## ðŸŒ Global Feature Importance (SHAP Bar Summary Plot)\n",
        "\n",
        "The bar plot illustrates the **average absolute impact** of each feature on the model's output across the entire dataset. The features are ranked from most important (top) to least important (bottom) based on the length of the bar, which corresponds to $\\text{mean}(\\vert \\text{SHAP value} \\vert)$.\n",
        "\n",
        "### Top 3 Most Important Features\n",
        "\n",
        "* **Study_Hours_Per_Day**: This is the most influential feature by a significant margin. Its impact on the model's prediction is the strongest overall.\n",
        "* **Sleep_Hours_Per_Day**: This is the second most important feature, indicating a substantial average impact on the model's output.\n",
        "* **Total_Activity_Hours**: This ranks third, showing a significant relationship with the predicted outcome, though its average impact is less than sleep hours.\n",
        "\n",
        "### Features with Moderate Impact\n",
        "\n",
        "These features have a noticeable, but smaller, average influence on the prediction:\n",
        "\n",
        "* **Stress_Level**: The fourth most important feature.\n",
        "* **Physical_Activity_Hours_Per_Day**: Ranks fifth.\n",
        "\n",
        "### Features with Minimal Impact\n",
        "\n",
        "The remaining features have the least average impact on the model's output, as their bars are the shortest:\n",
        "\n",
        "* **study_stress_ratio**\n",
        "* **Extracurricular_Hours_Per_Day**\n",
        "* **Social_Hours_Per_Day**\n",
        "* **Study_to_Sleep_Ratio**\n"
      ],
      "metadata": {
        "id": "rv-RhGg4gAek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LIME EXPLAINABILITY (Local Interpretable Model-agnostic Explanations)\n",
        "\n",
        "1. **Purpose:**\n",
        "LIME is a tool to explain predictions of any machine learning model (like Random Forest, Ridge, Neural Networks), even if the model is a â€œblack box.â€\n",
        "\n",
        "**Key idea**: Instead of trying to understand the whole model globally, LIME explains one prediction at a time (local explanation).\n",
        "\n",
        "2.** How it works (simple analogy)**:\n",
        "\n",
        " - Imagine your model is a very complicated machine.\n",
        "\n",
        " - You want to know why it made a single prediction.\n",
        "\n",
        " - LIME creates a simple, interpretable model (like a linear model) that approximates the black-box model just around that one prediction.\n",
        "\n",
        " - It tells you which features pushed the prediction up or down.\n",
        "\n",
        "3. **Steps LIME follows:**\n",
        "\n",
        " 1.Take the instance you want to explain (one row of data).\n",
        "\n",
        " 2.Generate similar data points around it (perturb the features slightly).\n",
        "\n",
        " 3.Ask the black-box model to predict these new points.\n",
        "\n",
        " 4.Fit a simple interpretable model (like a linear regression) on these perturbed points.\n",
        "\n",
        " 5.Use the simple model to see which features were most important for this prediction.\n",
        "\n",
        "4. **Key Points:**\n",
        "\n",
        "- LIME is local, not global. It explains one prediction at a time.\n",
        "\n",
        "- It works with any model: tree-based, neural network, or linear.\n",
        "\n",
        "- Output is easy to interpret: it shows which features increased or decreased the prediction.\n"
      ],
      "metadata": {
        "id": "ekmj5rShgY9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lime"
      ],
      "metadata": {
        "id": "hsDpBlrXfGv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lime\n",
        "import lime.lime_tabular\n",
        "\n",
        "np.random.seed(42)   # Fix numpy randomness\n",
        "\n",
        "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
        "    training_data=np.array(X_train_scaled),\n",
        "    feature_names=X_train.columns,\n",
        "    mode='regression',\n",
        "    random_state=42     # Fix LIME randomness\n",
        ")\n",
        "\n",
        "# Pick an index of the test student you want to explain\n",
        "i = 0\n",
        "student = X_test_scaled[i].reshape(1, -1)  # single sample\n",
        "\n",
        "# Generate explanation\n",
        "exp = explainer.explain_instance(\n",
        "    data_row=student[0],\n",
        "    predict_fn=best_model.predict,\n",
        "    num_features=len(X_train.columns)\n",
        ")\n",
        "\n",
        "# Print feature contributions\n",
        "exp.as_list()"
      ],
      "metadata": {
        "id": "TVP4FU3rgbnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Contributions (Local Explanation)\n",
        "\n",
        "The LIME explainer provides a list of features with their contribution to the prediction for the selected student. Each tuple shows:\n",
        "\n",
        "* The **feature condition** (value range or threshold)\n",
        "* The **effect** (how much it pushed the prediction up or down)\n",
        "\n",
        "### Output:\n",
        "\n",
        "| Feature Condition                               | Contribution |\n",
        "| ----------------------------------------------- | ------------ |\n",
        "| Stress_Level <= -0.48                           | -0.1823      |\n",
        "| Study_to_Sleep_Ratio <= -0.75                   | 0.0722       |\n",
        "| -0.81 < Study_Hours_Per_Day <= -0.04            | -0.0645      |\n",
        "| -0.01 < Sleep_Hours_Per_Day <= 0.89             | -0.0460      |\n",
        "| Extracurricular_Hours_Per_Day > 0.86            | -0.0398      |\n",
        "| -0.75 < Total_Activity_Hours <= 0.03            | 0.0163       |\n",
        "| -0.51 < study_stress_ratio <= 0.00              | -0.0158      |\n",
        "| Social_Hours_Per_Day <= -0.89                   | 0.0155       |\n",
        "| -0.09 < Physical_Activity_Hours_Per_Day <= 0.72 | -0.0002      |\n",
        "\n",
        "## Interpretation:\n",
        "\n",
        "* **Negative contribution**: Pushes the predicted outcome **lower** compared to the average prediction.\n",
        "* **Positive contribution**: Pushes the predicted outcome **higher**.\n",
        "\n",
        "### Key Insights for This Student:\n",
        "\n",
        "* **Stress_Level** had the largest negative impact (-0.1823), significantly lowering the predicted outcome.\n",
        "* **Study_to_Sleep_Ratio** contributed positively (0.0722), pushing the prediction upward.\n",
        "* **Study_Hours_Per_Day** and **Sleep_Hours_Per_Day** also slightly decreased the prediction.\n",
        "* Other features had smaller effects and were nearly negligible.\n",
        "\n",
        "âœ… **Summary:** LIME provides a **local explanation** showing which features drove this specific student's predicted value higher or lower relative to the model's expected output.\n"
      ],
      "metadata": {
        "id": "hNxs0bmRljte"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CLASSIFICATION METRICS"
      ],
      "metadata": {
        "id": "9Jao9ognl0DK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Classes from GPA This code performs hyperparameter tuning for a Ridge classification model using GridSearchCV. GridSearchCV then tests many different ridge settings and finds which combination gives the highest accuracy. i want this according to my model\n"
      ],
      "metadata": {
        "id": "Qw0xzAlRnEVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import RidgeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define GPA classes\n",
        "bins = [0, 2.0, 3.0, 3.3, 3.7, 4.0]          # adjust thresholds as needed\n",
        "labels = ['Fail', 'General', 'Second_lower','Second_upper','First_class']  # class names\n",
        "df['GPA_Class'] = pd.cut(df['GPA'], bins=bins, labels=labels)\n",
        "\n",
        "# Features and target\n",
        "X = df.drop(['GPA', 'GPA_Class'], axis=1)\n",
        "y = df['GPA_Class']\n",
        "\n",
        "# Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n"
      ],
      "metadata": {
        "id": "CRgeB9PskyLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RidgeClassifier GPA Classification Workflow\n",
        "\n",
        "Setup for predicting GPA classes using RidgeClassifier.\n",
        "\n",
        "## 1. Define GPA Classes\n",
        "\n",
        "```python\n",
        "bins = [0, 2.0, 3.0, 3.3, 3.7, 4.0]          # adjust thresholds as needed\n",
        "labels = ['Fail', 'General', 'Second_lower','Second_upper','First_class']  # class names\n",
        "  # Class labels\n",
        "df['GPA_Class'] = pd.cut(df['GPA'], bins=bins, labels=labels)\n",
        "```\n",
        "\n",
        "* **Purpose:** Convert continuous GPA values into discrete classes.\n",
        "* **Bins:** Define the ranges for each GPA class.\n",
        "* **Labels:** Name each class for interpretability.\n",
        "\n",
        "Example:\n",
        "\n",
        "* GPA 2.0 â†’ 'Low'\n",
        "* GPA 3.0 â†’ 'Medium'\n",
        "* GPA 3.8 â†’ 'High'\n",
        "\n",
        "## 2. Select Features and Target\n",
        "\n",
        "```python\n",
        "X = df.drop(['GPA', 'GPA_Class'], axis=1)\n",
        "y = df['GPA_Class']\n",
        "```\n",
        "\n",
        "* **X (features):** All columns except GPA and the newly created GPA_Class.\n",
        "* **y (target):** The GPA class we want to predict.\n",
        "\n",
        "## 3. Train-Test Split\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "```\n",
        "\n",
        "* **Purpose:** Split the dataset into training and testing sets.\n",
        "* **Test size:** 20% of the data is held out for evaluation.\n",
        "* **Random state:** Ensures reproducibility of the split.\n",
        "\n",
        "After this step:\n",
        "\n",
        "* `X_train`, `y_train` â†’ used to train the model.\n",
        "* `X_test`, `y_test` â†’ used to evaluate the model's performance.\n"
      ],
      "metadata": {
        "id": "DG2xbVEFtltF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "param_grid = {'alpha': [0.01, 0.1, 0.2, 0.5, 1.0, 5.0]}\n",
        "\n",
        "ridge = RidgeClassifier()\n",
        "grid_search = GridSearchCV(ridge, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_ridge = grid_search.best_estimator_\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best cross-validated accuracy: \", grid_search.best_score_)\n",
        "\n"
      ],
      "metadata": {
        "id": "hl7P6Cc4nttT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation\n",
        "\n",
        "* **param_grid:** Defines the regularization strength `alpha` values to test.\n",
        "* **GridSearchCV:** Performs 5-fold cross-validation to find the alpha with the highest accuracy.\n",
        "* **best_ridge:** The RidgeClassifier with the optimal alpha.\n",
        "\n",
        "**Output Interpretation:**\n",
        "\n",
        "* `Best Parameters: {'alpha': 5.0}` â†’ The model performs best with a very small regularization.\n",
        "* `Best cross-validated accuracy: 0.602` â†’ On average across folds, the model correctly classifies ~60.2% of students.\n"
      ],
      "metadata": {
        "id": "e3Op9OsvuJGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = best_ridge.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Weighted precision, recall, F1-score\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Classification Report\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "print(\"\\nAccuracy:\", accuracy)\n",
        "print(\"Precision (weighted):\", precision)\n",
        "print(\"Recall (weighted):\", recall)\n",
        "print(\"F1-score (weighted):\", f1)\n",
        "print(\"\\nClassification Report:\\n\", report)\n"
      ],
      "metadata": {
        "id": "9UQvRcfEoR1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RidgeClassifier GPA Classification Results\n",
        "\n",
        "After training a **RidgeClassifier** on GPA classes, we evaluate the model using several metrics.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Confusion Matrix\n",
        "\n",
        "```text\n",
        "Confusion Matrix:\n",
        " [[  0   0   0  10]\n",
        " [  0 119  18   4]\n",
        " [  0  61  39  43]\n",
        " [  0  12  20  73]]\n",
        "```\n",
        "\n",
        "* **Rows:** True GPA classes\n",
        "* **Columns:** Predicted GPA classes\n",
        "* **Interpretation:**\n",
        "\n",
        "  * The first row (`First_class`) was mostly misclassified (all 10 samples predicted incorrectly).\n",
        "  * `General` class has high correct predictions (119 out of 141).\n",
        "  * `Second_lower` has lower accuracy, many misclassified as `General` or `Second_upper`.\n",
        "  * `Second_upper` shows moderate performance (73 correctly predicted out of 105).\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Accuracy\n",
        "\n",
        "```text\n",
        "Accuracy: 0.579\n",
        "```\n",
        "\n",
        "* **Definition:** Overall fraction of correctly predicted samples.\n",
        "* **Interpretation:** About 58% of the GPA class predictions are correct.\n",
        "* **Observation:** Moderate accuracy; the model struggles with minority classes (`First_class`).\n",
        "\n",
        "---\n",
        "\n",
        "## 3.Weighted Precision, Recall, and F1-score\n",
        "\n",
        "```text\n",
        "Precision (weighted): 0.548\n",
        "Recall (weighted): 0.579\n",
        "F1-score (weighted): 0.543\n",
        "```\n",
        "\n",
        "* **Precision (weighted):** Average of precision across classes, weighted by support (number of samples per class). Measures correctness of positive predictions.\n",
        "* **Recall (weighted):** Average of recall across classes, weighted by support. Measures coverage of actual positive samples.\n",
        "* **F1-score (weighted):** Weighted harmonic mean of precision and recall.\n",
        "\n",
        "**Interpretation:**\n",
        "\n",
        "* Precision and F1-score are slightly lower than accuracy, reflecting misclassification of some classes.\n",
        "* The model performs better for `General` and `Second_upper` classes due to more samples.\n",
        "\n",
        "---\n",
        "\n",
        "## Classification Report\n",
        "\n",
        "```text\n",
        "Classification Report:\n",
        "               precision    recall  f1-score   support\n",
        "\n",
        " First_class       0.00      0.00      0.00        10\n",
        "     General       0.62      0.84      0.71       141\n",
        "Second_lower       0.51      0.27      0.35       143\n",
        "Second_upper       0.56      0.70      0.62       105\n",
        "\n",
        "    accuracy                           0.58       399\n",
        "   macro avg       0.42      0.45      0.42       399\n",
        "weighted avg       0.55      0.58      0.54       399\n",
        "```\n",
        "\n",
        "* **Support:** Number of samples in each class.\n",
        "* **Precision:** Correct predictions / total predicted for that class.\n",
        "* **Recall:** Correct predictions / total true samples of that class.\n",
        "* **F1-score:** Harmonic mean of precision and recall.\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "* `First_class` is poorly predicted due to very few samples (class imbalance).\n",
        "* `General` is the most accurately predicted class.\n",
        "* `Second_lower` has lower recall, meaning many samples were misclassified.\n",
        "* `Weighted avg` provides an overall summary considering class imbalance.\n",
        "\n",
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "* The RidgeClassifier achieves **moderate performance (accuracy ~58%)**.\n",
        "* Model struggles with **minority classes** (`First_class`) due to class imbalance.\n",
        "* Weighted metrics indicate better performance for larger classes (`General`, `Second_upper`).\n"
      ],
      "metadata": {
        "id": "7-24mnaGq9aL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Use your actual test labels and predictions\n",
        "confusion_m = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(confusion_m, annot=True, fmt='d', cmap='winter',\n",
        "            xticklabels=['Fail', 'General', 'Second_lower','Second_upper','First_class'],  # make sure these match your classes\n",
        "            yticklabels=['Fail', 'General', 'Second_lower','Second_upper','First_class'])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "38pfXiPNpLWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check class distribution in the test set\n",
        "print(y_test.value_counts())\n"
      ],
      "metadata": {
        "id": "wQNeDFiCrXgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Set Class Distribution\n",
        "\n",
        "```text\n",
        "GPA_Class\n",
        "Second_lower    143\n",
        "General         141\n",
        "Second_upper    105\n",
        "First_class      10\n",
        "Fail              0\n",
        "Name: count, dtype: int64\n",
        "```\n",
        "\n",
        "* **Purpose:** Check how the samples are distributed across GPA classes in the test set.\n",
        "* **Observations:**\n",
        "\n",
        "  * `Second_lower` and `General` are the most frequent classes.\n",
        "  * `First_class` is the least represented with only 10 samples.\n",
        "  * `Fail` class has no samples in the test set.\n",
        "* **Implication:** The model may struggle to predict minority classes accurately due to imbalance.\n",
        "\n",
        "##\n"
      ],
      "metadata": {
        "id": "LuGYtzP49Prh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ROC Curve (Multiclass)\n"
      ],
      "metadata": {
        "id": "9lS51usmuX2p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define classes in the same order as your labels\n",
        "classes = ['Fail', 'General', 'Second_lower','Second_upper','First_class']\n",
        "\n",
        "# Binarize the classes\n",
        "y_test_bin = label_binarize(y_test, classes=classes)\n",
        "y_pred_bin = label_binarize(y_pred, classes=classes)\n",
        "\n",
        "# Plot ROC for each class\n",
        "plt.figure(figsize=(8,6))\n",
        "for i, class_name in enumerate(classes):\n",
        "    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_pred_bin[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(f\"Class {class_name} - AUC: {roc_auc:.2f}\")\n",
        "    plt.plot(fpr, tpr, label=f\"{class_name} (AUC = {roc_auc:.2f})\")\n",
        "\n",
        "# Plot diagonal line for random guessing\n",
        "plt.plot([0,1], [0,1], 'k--')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve for Multi-class GPA Classification\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4lxCiJAAsBz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-class ROC and AUC\n",
        "\n",
        "* ROC curves are plotted for each GPA class to evaluate classifier performance.\n",
        "* **Classes and AUC values:**\n",
        "\n",
        "  * `Fail` - AUC: NaN (no samples in test set)\n",
        "  * `General` - AUC: 0.78\n",
        "  * `Second_lower` - AUC: 0.56\n",
        "  * `Second_upper` - AUC: 0.75\n",
        "  * `First_class` - AUC: 0.50\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "* AUC (Area Under the Curve) measures the classifier's ability to distinguish each class from others.\n",
        "\n",
        "* Higher AUC values indicate better performance.\n",
        "\n",
        "* `General` and `Second_upper` are classified relatively well, while `Second_lower` and `First_class` show weak performance.\n",
        "\n",
        "* `Fail` cannot be evaluated due to no test samples.\n",
        "\n",
        "* **Visualization:** ROC curves plot True Positive Rate (TPR) vs False Positive Rate (FPR) for each class, showing model performance across thresholds.\n"
      ],
      "metadata": {
        "id": "gjkTuM6g-D-p"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CA1whx1Bu9tY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}